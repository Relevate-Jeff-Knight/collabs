{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_cleaner.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arteric-Jeff-Knight/collabs/blob/master/text_cleaner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWo3ocbsYepm"
      },
      "source": [
        "# Run the first code block once to load libraries and define functions.  \n",
        "\n",
        "Ignore any output that isn't an error. Process takes a minute or so, but is finished when you see:\n",
        "\n",
        "<font color=\"green\">âœ” Download and installation successful</font><br>\n",
        "You can now load the model via spacy.load('en_core_web_sm')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_AezM4YZE1s",
        "cellView": "form"
      },
      "source": [
        "#@title <= Run first one time\n",
        "\n",
        "# Install needed libraries\n",
        "!pip install emoji contractions num2words\n",
        "\n",
        "# First the easy stuff.\n",
        "from google.colab import files\n",
        "import ipywidgets as widgets\n",
        "import io\n",
        "import re, string, unicodedata  # Import Regex, string and unicodedata.\n",
        "import numpy as np\n",
        "import pandas as pd  # Import pandas.\n",
        "import spacy\n",
        "from datetime import datetime\n",
        "import emoji\n",
        "import contractions  # Import contractions library.\n",
        "from num2words import num2words\n",
        "\n",
        "# We are only using spacy to lemmatize the content and to get stop words\n",
        "spacy.cli.download('en_core_web_sm')\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "\n",
        "compiled_ordinals = re.compile(r\"\\d+(?:st|nd|rd|th)\")\n",
        "compiled_acronyms = re.compile(r\"(?<!\\w)([A-Za-z])\\.\")\n",
        "compiled_dashes_to_to = re.compile('([0-9])-([0-9])')\n",
        "compiled_dashes_to_minus = re.compile(' -([0-9])')\n",
        "compiled_hyphens_to_spaces = re.compile('([A-Za-z])-([A-Za-z])')\n",
        "compiled_slashes_to_divided_by = re.compile('([0-9])/([0-9])')\n",
        "compiled_m_d_y = re.compile(r'\\d+/\\d+/\\d+')\n",
        "compiled_just_one_space = re.compile(r\"\\s\\s+\")\n",
        "\n",
        "\n",
        "# Define the normalization function\n",
        "def normalize(content: str, configs: dict) -> str:\n",
        "    # Set default configs if they are missing\n",
        "    if 'normalize_expand_symbols' not in configs:\n",
        "        configs['normalize_expand_symbols'] = '%=@'\n",
        "    if 'normalize_remove_punctuation' not in configs:\n",
        "        configs['normalize_remove_punctuation'] = '[^a-zA-Z0-9 ]'\n",
        "\n",
        "    # Convert emojis\n",
        "    if 'normalize_convert_emojis' not in configs or configs['normalize_convert_emojis']:\n",
        "        content = emoji.demojize(content)\n",
        "    # Remove non-ASCII\n",
        "    if 'normalize_to_ascii' not in configs or configs['normalize_to_ascii']:\n",
        "        content = unicodedata.normalize('NFKD', content).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    # Expand contractions\n",
        "    if 'normalize_expand_contractions' not in configs or configs['normalize_expand_contractions']:\n",
        "        content = contractions.fix(content)\n",
        "        content = content.replace(\" w/ \", \" with \")\n",
        "        # @todo more expansions like afk?\n",
        "    # remove URLs\n",
        "    if 'normalize_remove_urls' not in configs or configs['normalize_remove_urls']:\n",
        "        content = re.sub(r\"http\\S+\", \"\", content)\n",
        "    # Remove leading RT_\n",
        "    if 'normalize_remove_retweet' not in configs or configs['normalize_remove_retweet']:\n",
        "        content = re.sub(\"^[Rr][Tt] \", \"\", content)\n",
        "        # Remove leading @somename\n",
        "    if 'normalize_remove_tweeter' not in configs or configs['normalize_remove_tweeter']:\n",
        "        content = re.sub(\"^@[^ ]*\", \"\", content)\n",
        "    # If requested, remove all usernames\n",
        "    if 'normalize_remove_usernames' not in configs or configs['normalize_remove_usernames']:\n",
        "        content = re.sub(\"@[^ ]*\", \" \", content)\n",
        "    # If requested, expand dates like m/d/y and m/d to strings\n",
        "    if 'normalize_expand_dates' not in configs or configs['normalize_expand_dates']:\n",
        "        # it is impossible to tell whether 1/3 is a fraction or a date, so we won't even try\n",
        "        for date in set(compiled_m_d_y.findall(content)):  # only need to do once\n",
        "            try:\n",
        "                dto = datetime.strptime(date, '%d/%m/%Y')\n",
        "                day = dto.strftime('%-d')\n",
        "                do = num2words(day, to=\"ordinal\")\n",
        "                fulldate = dto.strftime(f'%B {do} %Y')\n",
        "                content = content.replace(date, fulldate)\n",
        "            except Exception as e:\n",
        "                try:\n",
        "                    dto = datetime.strptime(date, '%d/%m/%y')\n",
        "                    day = dto.strftime('%-d')\n",
        "                    do = num2words(day, to=\"ordinal\")\n",
        "                    fulldate = dto.strftime(f'%B {do} %Y')\n",
        "                    content = content.replace(date, fulldate)\n",
        "                except Exception as e:\n",
        "                    pass\n",
        "    # Expand some symbols\n",
        "    if 'normalize_expand_symbols' in configs and configs['normalize_expand_symbols']:\n",
        "        if '%' in configs['normalize_expand_symbols']:\n",
        "            content = content.replace(\"%\", \" percent \")\n",
        "        if '=' in configs['normalize_expand_symbols']:\n",
        "            content = content.replace(\"=\", \" equals \")\n",
        "        if '@' in configs['normalize_expand_symbols']:\n",
        "            content = content.replace(\"@\", \" at \")\n",
        "        # @todo more symbols\n",
        "    # Replace all the ordinals\n",
        "    if 'normalize_replace_ordinals' not in configs or configs['normalize_replace_ordinals']:\n",
        "        ordinals = compiled_ordinals.findall(content)  # find all in string\n",
        "        if ordinals:\n",
        "            ordinals = set(ordinals)  # reduce to unique values\n",
        "            for ordinal in ordinals:\n",
        "                content = content.replace(ordinal, num2words(ordinal[:-2], to=\"ordinal\") + ' ')\n",
        "    # Convert to lower case, then get rid of punctuation\n",
        "    if 'normalize_convert_to_lower' not in configs or configs['normalize_convert_to_lower']:\n",
        "        content = content.lower()\n",
        "        # Get rid of remaining punctuation\n",
        "    if 'normalize_remove_punctuation' in configs and configs['normalize_remove_punctuation']:\n",
        "        # if they aren't preserved, get rid of the periods in acronymns \n",
        "        # (so they don't get changed to spaces)\n",
        "        if '.' not in configs['normalize_remove_punctuation']:\n",
        "            content = compiled_acronyms.sub(r'\\1', content)\n",
        "        # update all hyphens between words to spaces \n",
        "        content = compiled_hyphens_to_spaces.sub(r'\\1 \\2', content)\n",
        "        # and all the number-number to number to number\n",
        "        content = compiled_dashes_to_to.sub(r'\\1 to \\2', content)\n",
        "        # any remaining space dash number is probably a minus sign\n",
        "        content = compiled_dashes_to_minus.sub(r'minus \\1', content)\n",
        "        # get rid of anything left over\n",
        "        content = re.sub(configs['normalize_remove_punctuation'], \" \", content)\n",
        "    # Replace numbers with words\n",
        "    if 'normalize_numbers_to_words' not in configs or configs['normalize_numbers_to_words']:\n",
        "        # Consider just splitting on the space to KISS\n",
        "        words = []\n",
        "        for word in content.split(' '):\n",
        "            try:\n",
        "                if word.isnumeric():\n",
        "                    if int(word) < 999:\n",
        "                        word = num2words(word)\n",
        "            except Exception as e:\n",
        "                pass\n",
        "            words.append(word)\n",
        "        # put everything back together\n",
        "        content = ' '.join(words)\n",
        "    # Reduce multiple spaces\n",
        "    content = compiled_just_one_space.sub(\" \", content).strip()\n",
        "    return content\n",
        "\n",
        "\n",
        "def lemmatize(sentence: str):\n",
        "    keepers = []\n",
        "    for token in nlp(sentence):\n",
        "      if token.lemma_[0] != '-':\n",
        "        keepers.append(token.lemma_)\n",
        "      else:\n",
        "        keepers.append(token.text)\n",
        "    sentence = \" \".join(keepers)\n",
        "    return sentence.replace(' - ','-')\n",
        "\n",
        "\n",
        "def split_df_into_data_and_configs(uploaded, defaults=None, config_name: str = 'config'):\n",
        "    # Put uploaded file into dataframe\n",
        "    if defaults is None:\n",
        "        defaults = {}\n",
        "\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    df = pd.read_csv(io.BytesIO(uploaded[filename]), header=None)\n",
        "\n",
        "    # Get everything with 'configs' in first coumn\n",
        "    configs = df[df[0] == config_name]\n",
        "    # Build a dictionary from the key in the second column with values from the third\n",
        "    defaults.update(dict(zip(configs[1], configs[2])))\n",
        "\n",
        "    # Everything else that isn't a config, is data\n",
        "    data = df[df[0] != config_name].reset_index(drop=True)\n",
        "    # Assume that the first row is the column names now that configs are gone\n",
        "    data.columns = data.iloc[0]\n",
        "    # Drop the row with the column names\n",
        "    data.drop(df.index[0], inplace=True)\n",
        "    # Reset the index, so zero works below\n",
        "    data = data.reset_index(drop=True)\n",
        "\n",
        "    # Validate the column name configs\n",
        "    data_column_list = list(data.columns)\n",
        "\n",
        "    if 'normalize_col_in' not in defaults or defaults['normalize_col_in'] not in data_column_list:\n",
        "        # With nothing defined or garbage, use first column\n",
        "        if defaults['normalize_col_in'].capitalize() in data_column_list:\n",
        "            defaults['normalize_col_in'] = defaults['normalize_col_in'].capitalize()\n",
        "        else:\n",
        "            defaults['normalize_col_in'] = data_column_list[0]\n",
        "\n",
        "    if 'normalize_col_out' not in defaults:\n",
        "        defaults['normalize_col_out'] = 'clean_text'\n",
        "\n",
        "    if 'normalize_col_lem' not in defaults:\n",
        "        defaults['normalize_col_lem'] = 'lemmatized'\n",
        "\n",
        "    if 'normalize_file_out' not in defaults:\n",
        "        defaults['normalize_file_out'] = '-cleaned'\n",
        "\n",
        "    if 'normalize_drop_dupes' not in defaults:\n",
        "        defaults['normalize_drop_dupes'] = True\n",
        "\n",
        "    if not isinstance(defaults['normalize_drop_dupes'], bool) \\\n",
        "            and defaults['normalize_drop_dupes'].lower() in ['false', '0', 0]:\n",
        "        defaults['normalize_drop_dupes'] = False\n",
        "\n",
        "    if 'normalize_output_filename' not in defaults:\n",
        "        defaults['normalize_output_filename'] = filename.replace('.csv', f\"{defaults['normalize_file_out']}.csv\")\n",
        "\n",
        "    return data, defaults\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmBiXSLf1orU"
      },
      "source": [
        "\n",
        "# Next, run the following code block to upload a file for processing\n",
        "\n",
        "If you want to process multiple files, start here for each one, no need to run the first block again and again.\n",
        "\n",
        "Processing is mostly handled by default values, but if you need to override them, configuration is handled by passing values in the input file. See **How to Configure** below for instructions.\n",
        "\n",
        "The code will ask you to choose a file to import and then it will download the results when it is finished."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQZKIC06cJTz",
        "cellView": "form"
      },
      "source": [
        "# @title <= Run once for every file to clean\n",
        "\n",
        "defaults = {\n",
        "    'normalize_col_in': 'content',\n",
        "    'normalize_col_out': 'clean_text',\n",
        "    'normalize_col_lem': 'lemmatized',\n",
        "    'normalize_file_out': '-cleaned',\n",
        "    'normalize_drop_dupes': True\n",
        "}\n",
        "\n",
        "df, configs = split_df_into_data_and_configs(files.upload(), defaults)\n",
        "\n",
        "print('---------- configs ----------')\n",
        "for key in configs:\n",
        "    print('   ', key, ':', configs[key])\n",
        "print('-----------------------------')\n",
        "display(df)\n",
        "\n",
        "# Remove duplicate rows\n",
        "num_rows = df.shape[0]\n",
        "if configs['normalize_drop_dupes']:\n",
        "    df.drop_duplicates(configs['normalize_col_in'], inplace=True)\n",
        "    print(f'Processing {df.shape[0]} rows after deleting {num_rows - df.shape[0]} duplicates:\\n')\n",
        "else:\n",
        "    print(f'Processing {df.shape[0]} rows:\\n')\n",
        "\n",
        "# Apply the normalization function to the input file\n",
        "df[configs['normalize_col_out']] = df.apply(lambda x: normalize(x[configs['normalize_col_in']], configs), axis=1)\n",
        "df[configs['normalize_col_lem']] = df.apply(lambda x: lemmatize(x[configs['normalize_col_out']]), axis=1)\n",
        "\n",
        "print('Finished Processing\\n')\n",
        "\n",
        "print(f\"Saving locally to {defaults['normalize_output_filename']}\\n\")\n",
        "# This code downloads the result to your local machine.\n",
        "df.to_csv(configs['normalize_output_filename'], index=False)\n",
        "files.download(configs['normalize_output_filename'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yddOzvvMG-7j",
        "cellView": "form"
      },
      "source": [
        "#@title <= Optional: for debugging, list 10 random results\n",
        "for idx in np.random.choice(df.shape[0], replace = True, size = 5):\n",
        "    print('----------')\n",
        "    print(df.loc[idx][configs['normalize_col_in']])\n",
        "    print('-')\n",
        "    print(df.loc[idx][configs['normalize_col_out']])\n",
        "    print('-')\n",
        "    print(df.loc[idx][configs['normalize_col_lem']])\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LrljB3OkP44"
      },
      "source": [
        "\n",
        "# **About the Normalization Function**\n",
        "\n",
        "### Here we define the steps to normalize the text:\n",
        "\n",
        "Normalization is done in a specific order to try and resolve all the predictable cases before most of the punctuation is removed. \n",
        "\n",
        "- If `normalize_convert_emojis`, convert all emojis to alias string equivalents\n",
        "    - e.g. ðŸ‘  becomes `:thumbs_up:`\n",
        "    - Subsequent punctuation removal changes `:thumbs_up:` to \"thumbs up\"\n",
        "- If `normalize_to_ascii`, convert Unicode to ASCII and then back again\n",
        "    - This removes all emojis and accents and other garbage\n",
        "    - Converts back to unicode because later operations expect it\n",
        "- if `normalize_expand_contractions`, expand contractions\n",
        "    - For consistent grammar, expand \"it's\" to \"it is\", etc.\n",
        "    - Future removal of punctation would change contractions to nonsense\n",
        "- If `normalize_remove_urls`, Remove URLs\n",
        "    - They are not words\n",
        "- If `normalize_remove_retweet`, Remove 'RT ' from the start\n",
        "    - Many tweets begin with \"RT \" for retweet\n",
        "    - Even if string is not a tweet, \"RT \" is nonsense and can safely be eliminated.\n",
        "- If `normalize_remove_tweeter`, Remove '@username: ' from start of string\n",
        "    - Even tweets that aren't retweets begin with \"@username: \" which is garbage\n",
        "- If `normalize_remove_usernames`, Remove all usernames beggining with '@'\n",
        "    - If not removed they are handled like proper nouns, but punctuation removal can cause unintended consequences\n",
        "- If `normalize_expand_dates`, change m/d/y style dates to strings\n",
        "    - Month is converted to name\n",
        "    - Date is converted to orinal value (e.g. \"2\" becomes \"second\")\n",
        "    - Year remains a number but is padded to four digits\n",
        "- Replace any symbols found in `normalize_expand_symbols` with their word equivalend.\n",
        "    - \"%\" becomes \"percent\"\n",
        "    - \"=\" becomes \"equals\"\n",
        "    - \"@\" becomes \"at\"\n",
        "    - Dash and Minus are impossible to differentiate\n",
        "- If `normalize_replace_ordinals`, change ordinals to strings\n",
        "  - e.g. \"2nd\" becomes \"second\"\n",
        "- If `normalize_convert_to_lower`, convert all letters to lower case\n",
        "  - Prevents system from considering \"Word\" and \"word\" as two different words\n",
        "- If `normalize_remove_punctuation` is `False` do nothing, otherwise value is a regular expression of characters to **keep**\n",
        "    - Defaults to `[^a-zA-Z0-9 ]`, anything that isn't a letter or a number is replaced with a space. \n",
        "    - Keep all numbers and letters (capitalization may be preserved)\n",
        "    - Changing this configuration requires an understanding of [Regular Expressions](https://docs.python.org/3/howto/regex.html)\n",
        "- If `normalize_numbers_to_words`, changes all numbers less than one thousand to their English equivalent\n",
        "  - e.g. \"42\" becomes forty-two\n",
        "    - Note this may reintroduce the hyphen!\n",
        "- Change all spaces to single space and remove all leading and trialing spaces\n",
        "  - Always necessary, no need to configure\n",
        "\n",
        "\n",
        "# **How to Configure**\n",
        "\n",
        "To add configurations to a file, put the value 'config' in the first column (no matter what the header) and the configuration key name in the second, and the value to be set in the third (anything beyond that can be ignored). Durding processing these rows will be separated from the data and not included in the returned file. For convenience, they can appear anywhere in the incoming file: before the headers, at the end, anywhere in between or even mixed among the data.\n",
        "\n",
        "The possible configuration keys and their default values are:\n",
        "\n",
        "| Key | Defualt | Notes |\n",
        "|--------------|:-----------|:------|\n",
        "| `normalize_col_in` | Content | *The exact, case sensitive name of column in the incoming file to process* |\n",
        "| `normalize_col_out` | clean_text | *The name of column to add the processed content* |\n",
        "| `normalize_col_lem` | lemmatized | *The name of column to add the lemmatized processed content* |\n",
        "| `normalize_file_out` | -cleaned | *The text to add to the filename that is returned* |\n",
        "| `normalize_output_filename` | | If passed, this value overrides the value calulated by applyting `file_out` to the uploaded filename |\n",
        "| `normalize_drop_dupes` | `True` | *If this is set to True, then rows that duplicate content are dropped* | \n",
        "| `normalize_convert_emojis` | `True` | *Converts emojis to word strings, if `False`, `normalize_to_ascii` eliminates them entirely* |\n",
        "| `normalize_to_ascii` | `True` | *Reduces unicode to ASCII or eliminates character where no conversion is possible* |\n",
        "| `normalize_expand_contractions` | `True` | *Changes all contractions to long form* |\n",
        "| `normalize_remove_urls` | `True` | *Removes URLs* |\n",
        "| `normalize_remove_retweet` | `True` | *Eliminates any leading 'RT ' characters* |\n",
        "| `normalize_remove_tweeter` | `True` | *Removes the first @\\<username> string* |\n",
        "| `normalize_remove_usernames` | `True` | *Removes all @\\<username> strings* |\n",
        "| `normalize_expand_dates` | `True` | *Conerts m/d/y to month and day names and numeric year * |\n",
        "| `normalize_expand_symbols` | %=@ | *`False` or empty to do nothing, otherwise list of symbols to expand to words, e.g. % to 'percent'* |\n",
        "| `normalize_replace_ordinals` | `True` | *Changes ordinals to words, e.g. 2nd to 'second'* |\n",
        "| `normalize_convert_to_lower` | `True` | *Converts all text to lower case* |\n",
        "| `normalize_remove_punctuation` | [^a-zA-Z0-9 ] | *Removes remaining punctuation* |\n",
        "| `normalize_numbers_to_words` | `True` | *Changes numbers less than one thousand to text, e.g. 27 to 'twenty-seven'* |\n",
        "\n",
        "- If the value in `normalize_col_in` does not match any header in the uploaded file, the content in the first column will be processed.\n",
        "- If the value in `normalize_col_out` or `normalize_col_lem` matches an existing column, it will be overwritten.\n",
        "- The value in `normalize_file_out` will be inserted between the base filename and the '.csv' of the uploaded file to create `normalize_output_filename` unless a value is passed as a configuration.\n",
        "\n",
        "\n",
        "# **Sample Configuration to Copy and Paste**\n",
        "\n",
        "No need to add them all to a file, just the ones you want to change:\n",
        "\n",
        "```\n",
        "normalize_col_in,content\n",
        "normalize_col_out,clean_text\n",
        "normalize_col_lem,lemmatized\n",
        "normalize_file_out,-cleaned\n",
        "normalize_output_filename,custom_filename.csv\n",
        "normalize_drop_dupes,True\n",
        "normalize_convert_emojis,True\n",
        "normalize_to_ascii,True\n",
        "normalize_expand_contractions,True\n",
        "normalize_remove_urls,True\n",
        "normalize_remove_retweet,True\n",
        "normalize_remove_tweeter,True\n",
        "normalize_remove_usernames,True\n",
        "normalize_expand_dates,True\n",
        "normalize_expand_symbols,\"%=@\"\n",
        "normalize_replace_ordinals,True\n",
        "normalize_convert_to_lower,True\n",
        "normalize_remove_punctuation,\"[^a-zA-Z0-9 ]\"\n",
        "normalize_numbers_to_words,True\n",
        "```\n"
      ]
    }
  ]
}